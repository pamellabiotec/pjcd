{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JIhoKG8Lpuu"
   },
   "source": [
    "# Introdução a Machine Learning\n",
    "\n",
    "## 5.2 - Introdução ao Tensor Flow\n",
    "\n",
    "Olá\n",
    "\n",
    "Nesta aula, iremos conhecer o Tensorflow que é uma biblioteca da aprendizado de máquina e Deep Learning para utilização de redes neurais. Embora também possamos utilizar a biblioteca Scikit-learn para a construção de redes neurais. O Tensorflow oferece uma alternativa mais complexa, robusta e que permite a construção de uma rede com mais camadas ou mais opções.\n",
    "\n",
    "Então, o Tensorflow é extremamente importante na formação de quem está começando a trabalhar com aprendizaado de máquina. \n",
    "\n",
    "O Tensorflow foi pensado pelo Google Brain, uma divisão do Google incumbida da inteligência artificial da empresa. É uma biblioteca que foi pensada, no início, para computação numérica. E quando pensamos em todos os modelos de aprendizado de máquina como computação numérica, sobretudo, multiplicação de matrizes por vetores. O Tensorflow vem bem a calhar para esta finalidade. \n",
    "\n",
    "O Tensorflow foi pensado para rodar em CPUs, ou processadores normais, em GPUs, ou seja, placas de vídeo, uma vez que as redes neurais trabalham com multiplicações de matrizes por vetores. Como as placas de vídeos são especializadas em renderizar imagens. E imagens são, de fato, matrizes bem grandes, faz sentido rodarmos redes neurais em placas de vídeo. E o Tensorflow vem com  uma biblioteca para ser preparado para rodar em placas de vídeo. E, também, em dispositivos móveis, considerando as arquiteturas destes.\n",
    "\n",
    "O Tensorflow é capaz de ser rodado em diversos ambientes e foi desenvolvido para aprendizado de máquina e Deep Learning. Mas, não somente isso, ele foi pensado para computação numérica em geral. \n",
    "\n",
    "Algumas das empresas que utilizam o Tensorflow:\n",
    "\n",
    "![imagem.png](imagem/figura_5.2_1.png)\n",
    "\n",
    "O Tensorflow é uma biblioteca que está sendo bastante utilizada pela sua praticidade, mas, graças a uma parceria com o Keras, a utilização do Tensorflow ficou um pouco mais fácil. \n",
    "\n",
    "![imagem.png](imagem/figura_5.2_2.png)\n",
    "\n",
    "Uma vez que não precisávamos lidar com uma parte extremamente técnica do Tensorflow. \n",
    "\n",
    "Na próxima aula, iremos falar sobre o aspecto técnico do Tensorflow e como utilizá-lo para a construção das nossas redes.\n",
    "\n",
    "## Preparando o conjunto de dados\n",
    "\n",
    "Nessa aula iremos utilizar os primeiros passos do Tensorflow e do Keras. \n",
    "\n",
    "![imagem.png](imagem/figura_5.2_3.png)\n",
    "\n",
    "Keras era uma biblioteca que era utilizada como uma espécie de Front-end de outras bibliotecas de aprendizado de máquina, ou seja, o Keras era utilizando para facilitar a vida de quem queria utilizar bibliotecas mais complexas como era o caso do Tensorflow. \n",
    "\n",
    "A partir da versão 1.4, o **Tensorflow** incorpora o **Keras** em seus pacote de bibliotecas. Então, o Keras facilitou bastante a vida dos usuários do Tensorflow de modo que construir um modelo se tornou algo mais simples. \n",
    "\n",
    "Nessa aula, iremos utilizar o **Tensorflow**, o **Keras** e o **NumPy** como bibliotecas principais para manipular os dados que iremos utilizar durante esta aula. \n",
    "\n",
    "Caso, seja necessário instalar a biblioteca Tensorflow, digite o comando no jupyter notebook:\n",
    "\n",
    "``\n",
    "conda install -c anaconda tensorflow\n",
    "``\n",
    "Pronto!\n",
    "\n",
    "``\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "``\n",
    "\n",
    "O conjunto de dados utilizado nesta aula será carregado da base do Keras:\n",
    "\n",
    "``\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "``\n",
    "\n",
    "O nome deste conjunto de dados é \"fashion_mnist\". O \"mnist\" original era um conjunto de dados constituuído por dígitos escritos à mão. E o objetivo desse conjunto de dados, do \"mnist\" era tentar construir modelos que fossem capazes de reconhecer esses dígitos escritos à mão. Para variar um pouco, foi criado o \"fashion_mnist\" que é uma variação, dez categorias de rótulos diferentes. Várias imagens delas. E o objetivo é construir um modelo capaz de classificá-las corretamente. Em seguida, iremos separar o nosso conjunto de dados em treino e teste. Ou seja, essa função ``load_data()`` já efetua a separação do nosso conjunto de dados em um conjunto de \"train\" e \"test\", com os seus respectivos rótulos.\n",
    "\n",
    "E podemos conferir na linha seguinte, o tamanho dos nossos conjuntos de dados, utilizando os comandos:\n",
    "\n",
    "``\n",
    "train_images.shape\n",
    "\n",
    "len(train_labels)\n",
    "``\n",
    "\n",
    "Ao rodar o comando, teremos o resultado que a variável \"train_images\", um tensor 3D, onde nós temos 60 mil elementos de \"train\", cada um deles como uma matriz de 28x28. Então, o resultado desse shape será: (60000, 28, 28). O comando seguinte é para conferir o tamanho do nosso rótulo de treino. É esperado que os rótulos de \"train\" seja igual a \"train_images\". Ou seja, 60 mil. \n",
    "\n",
    "Para isso, iremos utilizar a nossa biblioteca **NumPy** que é uma biblioteca para tratar justamente com tensores e utilizaremos da seguinte maneira:\n",
    "\n",
    "``\n",
    "np.unique(train_labels)\n",
    "``\n",
    "\n",
    "Ao executar este comando, nós iremos ter todos os _labels_ que aparecem neste conjunto. Então, teremos:\n",
    "\n",
    "``\n",
    "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n",
    "``\n",
    "\n",
    "Dessa maneira, conseguimos descobrir quais são os rótulos de \"train\" e nós temos pelo menos um rótulo para cada uma dessas 10 classes. Ou seja, os 60 mil rótulos estão distribuídos nessas 10 classes. Também podemos efetuar no conjunto de \"test\" para verificar o seu tamanho.\n",
    "\n",
    "``\n",
    "test_images.shape\n",
    "``\n",
    "\n",
    "Ao executar este comando, iremos ter o resultado: ``(10000, 28, 28)`` Isso significa que temos 10 mil imagem para \"teste\" de 28x28. Enquanto que para \"train\" temos 60 mil. Em seguida, vamos conferir os rótulos:\n",
    "\n",
    "``\n",
    "len(test_labels)\n",
    "``\n",
    "\n",
    "E como para verificar quais são os rótulos de \"test\". \n",
    "\n",
    "``\n",
    "np.unique(test_labels)\n",
    "``\n",
    "\n",
    "Para finalizar essa preparação de dados, nós podemos verificar que as nossas imagens de \"train\" e as imagens de \"test\" são compostas por uma matriz de dimenões 28x28. E cada um desses elementos da matriz, ele é, exatamente, um pixel com valores entre 0 e 255. Onde 0 é a ausência de cor do pixel e 255 é a intensidade máxima do pixel.\n",
    "\n",
    "Em alguns modelos, tais como a regressão linear e as próprias redes neurais, eles se beneficiam de atributos escalados. Esses modelos utilziam uma técnica de gradiente para otimização, ou seja, de derivada. Portanto, quando esses valores estão em escala pequena, como entre 0 e 1, esses modelos experimentam uma performance melhor. como todos os nossos pixels estão com valores entre 0 e 255. Nós podemos dividir as nossas matrizes por 255. Dessa maneira, o pixel com maior intensidade que é o de 255 será igual a 1. O pixel de menor intensidade que é 0, continuará sendo 0. E todos os valores intermediários estarão na escala entre 0 e 1. Assim, estamos utilizando um \"MinMaxScaler\" chamado explicitamente. Então, para isso, iremos utilizar:\n",
    "\n",
    "``\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0\n",
    "``\n",
    "\n",
    "Dessa maneira, estamos forçando o resultado ser um ponto flutuante e todos os valores, tanto das imagens de \"train\" quando das de \"test\" estarão entre 0 e 1. \n",
    "\n",
    "Com isso, teremos preparado o nosso conjunto de dados para utilizar a nossa rede reural. Esta construção, esta preparação do conjunto de dados é tão importante quanto a implementação do modelo. Uma vez que dados que não estejam correntamente alinhados ou preparados podem gerar modelos ruins. Ou modelos cuja avaliação não é confiável. Então, nós temos um modelo que é capaz de ser executado de uma maneira bem confiável. Uma vez que temos uma separação entre \"train\" e \"test\" para avaliação e estamos normalizando o nosso conjunto de dados. \n",
    "\n",
    "\n",
    "## Construção e Avaliação do Modelo TensorFlow\n",
    "\n",
    "\n",
    "Nesta aula, iremos aprender a construir o nosso modelo e como avaliá-lo, de acordo com a métrica de acurácia.  Uma vez que estamos trabalhando com uma classificação.\n",
    "\n",
    "Inicialmente, já tendo importado as bibliotecas Keras, tensorflow e NumPy. Nós iremos executar o código a seguir:\n",
    "\n",
    "Mas iremos , primeiramente, definir a **arquitetura** do nosso modelo. Iremos colocar o seguinte comando:\n",
    "\n",
    "``\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "``\n",
    "\n",
    "Entre as linhas 1 e 2, até aí, nós estamos construindo um modelo que é sequencial. Ou seja, cujas camadas são uma em sequência da outra, o resultado de uma vai pra a próxima. Nesta primeira camada que instruímos, estamos utilizando uma \"layer\", que é camada, o Keras, de \"Flatten\"; ou seja, então, estamos pegando nossa matriz que tem tamanho 28x28. E a traduzindo em um vetor de 28 vezes 28. Porque esse modelo de rede neural a qual estamos utilizando no momento, ela não é capaz de trabalhar com matrizes. Logo, nosso objetivo será transformar a nossa matriz em um vetor. Simplemente concatenando, as linhas dessa matriz. Essa função \"**Flatten**\" já faz isso para nós. E nós passamos o tamanho do vetor de entrada com o parâmetro: ``input_shape=(28, 28)``.\n",
    "Após essas primeiras linhas, nós iremos colocar: \n",
    "\n",
    "``  \n",
    "  keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "``\n",
    "\n",
    "Nessas duas últimas camadas que foram ditas aqui. Nós estamos aplicando duas camadas densas, as camadas comuns de rede neurais nas quais todos os neurônicos da camada anterior se ligam aos neurônios da camada atual. E utilizamos uma função de ativação para cada uma dessas camadas onde, na primeira, estamos utilizando a \"ReLU\" que ReLU é uma função com qual qualquer número negativo se torna 0 e se o número for positivo mantém-se o número. Ou seja, essa função evita números negativos. E, por fim, uma última camada densa de 10 neurônios com a ativação \"**Softmax**\" que é uma função boa para classificação. \n",
    "\n",
    "Se formos perceber, estamos usando na última camada densa, 10 neurônios. É a última camada, uma camada de saída.\n",
    "\n",
    "Se notarmos, nós temos 10 clases no nosso conjunto de dados. Bate exatamente com o número de classes. Devemos sempre tomarmos esse cuidado com a última camada da nossa rede neural que deve bater com a quantidade de classes dos nosso conjunto de dados.\n",
    "\n",
    "Após a construção desse modelo, podemos utilizar o seguinte comando: `` model.summary()`` Esse comando nos dirá quantos pesos nós temos na nossa rede neural e a estrtutura da mesma. \n",
    "\n",
    "Agora, iremos compilar nosso modelo com seguinte código:\n",
    "\n",
    "``\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "``\n",
    "\n",
    "Em seguida, iremos treinar nosso modelo:\n",
    "\n",
    "``\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "``\n",
    "\n",
    "Essa função \"**epochs**\" significa época, ela defini a quantidade de interações nosso modelo terá. Essa quantidade pode varia de acordo com conjunto de dados. Para este conjunto de dados, só cinco epochs já bastam. Ao executar esse comado, veremos uma representação gráfica da execução de cada época sendo treinada, demonstração do valor de erros e de acurácia sendo identificada. \n",
    "\n",
    "Dessa maneira, nosso modelo tem 89,04% de acurácia para o conjunto de dados de \"train\". E essa quantidade de época foi escolhida pelo critério que podemos perceber na evolução das épocas: da primeira para segunda época, tivemos um aumento de 4% de acurácia. Ou seja, mais época poderia ocorrer um _overfitting_. Poderíamos ter uma métrica muito boa para treinamento e mas métricas não tão boas para teste.\n",
    "\n",
    "\n",
    "Por fim, podemos avaliar o nosso conjunto de dados sobre os nossos dados de \"test\". Para isso, usaremos o seguinte comando:\n",
    "\n",
    "``\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "``\n",
    "\n",
    "Com isto, iremos avaliar  \"test_images\" e \"test_labels\", ou seja, avaliar o conjunto de imagens e seus rótulos de teste. E irá guardar o valor de erro na variável \"test_loss\" e de acurácia na variável \"test_acc\".\n",
    "\n",
    "Com isso,conseguimos treinar o nosso modelo, defini-lo e avaliá-lo de uma maneira simples. \n",
    "\n",
    "Se quisermos avaliar individualmente, podemos utilizar o método \"predict\":\n",
    "\n",
    "``\n",
    "predictions = model.predict(test_images)\n",
    "``\n",
    "Que irá criar uma predição para cada uma das imagens de \"test\". Depois, imprimir o primeiro vetor dessa predição com o seguinte comando: \n",
    "\n",
    "``\n",
    "predictions[0]\n",
    "``\n",
    "\n",
    "Nós teremos uma _array_ com 10 elementos. Cada um deles com números em notação científica. Todos eles bem próximo a 0, são valores bem pequenos com exceção de um deles. A nossa classificação se dará pelo maior valor de predição que teremos dentro esses \"predictions\".\n",
    "\n",
    "Ao invés de olhar para cada uma desses valores, poderemos utilizar o NumPy para verificar o índice cujo elemento é o máximo. Dessa maneira, nós saberemos qual foi a predição da rede neural. Então, iremos utilizar:\n",
    "\n",
    "\n",
    "``\n",
    "np.argmax(predictions[0])\n",
    "``\n",
    "\n",
    "Ao executar isso, iremos perceber que foi o elemento de índice 9 que é o mairo de todos. Ou seja, temos a classe 9. \n",
    "\n",
    "Se verificamos o nosso rótulo, veremos que é um elemento da classe 9.\n",
    "\n",
    "De novo, iremos utilizar o NumPy para dimensionar nossa imagem em vetor:\n",
    "\n",
    "``\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M95hpvpELpu_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k4FAffEwMBKZ"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtHW6dfqMMmu",
    "outputId": "3532d3fc-ce0b-4223-f0b4-a8f76c107bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xc62Vy2AM6OG",
    "outputId": "8d0e76a7-0b9f-4944-8a85-b34531854507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GP_5_PCINt1W",
    "outputId": "69399609-aec2-47a3-ce7e-7651c59e9994"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lahUgyhPnRU",
    "outputId": "7bc350a6-513d-4852-f58e-ce6124db97ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HT5z_aoaP-KQ",
    "outputId": "67c885ee-cf6b-45e1-a5d2-74d973e604de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNtb1a8RVbNQ",
    "outputId": "14750a93-5931-441a-dc1f-c66db1a7b47f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDe6QKJIVq6y",
    "outputId": "47dd81be-fab7-48e7-aa42-b324a3c611c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2kzFvY3Vikp"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mm2Y7PwV_Jm",
    "outputId": "2fe51937-b091-488a-d537-28431027621f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHgwR1_0vQb2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqyVUviRV_Gm",
    "outputId": "5ffe63d3-7bbf-416c-dc8f-accfb050879a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6394 - accuracy: 0.7784\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3821 - accuracy: 0.8617\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3394 - accuracy: 0.8736\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3109 - accuracy: 0.8831\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2884 - accuracy: 0.8944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe4428c4dd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH5CFeXPYpVm",
    "outputId": "b390e0c2-3d00-408d-b3a1-9512b8e1f220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3476 - accuracy: 0.8801\n",
      "Test accuracy: 0.8801000118255615\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iSllt2EaNwJ"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eL7ct6aSaRcM",
    "outputId": "f0a6b668-0e66-4475-8714-bec8df3a153c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1390575e-08, 5.4105616e-08, 3.6913949e-07, 5.0269651e-09,\n",
       "       9.0455183e-07, 7.1448006e-02, 1.0816774e-07, 5.0263945e-02,\n",
       "       1.5678692e-05, 8.7827080e-01], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uibfhla6aVyQ",
    "outputId": "27526520-2eab-4102-916e-bc75a7c81fcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTEsC3GeadZe",
    "outputId": "73f4ad69-da38-457e-e717-9f4d04a4ac89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0Z17_uOhXf4",
    "outputId": "c073f4c7-9181-4af0-96e9-0c3bc762560e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "img = test_images[0]\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgQQohSxhfry",
    "outputId": "56c912ca-f6c2-4214-a17f-5a9a15b867a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBHKn9eLhn2G",
    "outputId": "903ca80a-e801-4bc9-dca4-7b183bf9ed07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1390675e-08 5.4105715e-08 3.6913949e-07 5.0269557e-09 9.0455183e-07\n",
      "  7.1448036e-02 1.0816754e-07 5.0263930e-02 1.5678705e-05 8.7827080e-01]]\n"
     ]
    }
   ],
   "source": [
    "predictions_single = model.predict(img)\n",
    "\n",
    "print(predictions_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGzZtT5HLpvC",
    "outputId": "351b516e-49da-409b-897f-632a7d478eaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions_single[0])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Aula_5.2 - Introdução ao Tensor Flow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
