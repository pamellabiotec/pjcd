{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a Machine Learning - Aula 2.1\n",
    "\n",
    "## Introdução ao K-Nearest Neighbor - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olá Pessoal!\n",
    "\n",
    "Nesta aula, vocês irão conhecer K-Nearest Neighbor - KNN, um dos modelo de Machine Learning que traduzido seu nome do inglês significa os \"K\"s **vizinhos mais próximos**.É um modelo muito intuitivo, pois se vale da ideia simples de classificar elementos ou executar a regressão de alguns elementos aa partir da distância entre eles.\n",
    "\n",
    "Imagine que você precise desenvolver um sistema que precise calcular a semelhança entre elementos. O seu resultado é a partir da semelhança. E essa semelhança pode ser calculada a partir de uma distância. Então, neste caso, o modelo de KNN aparece como uma boa alternativa. Pois resolve facilmente esse problema uma vez que as métricas de distância coneguem calcular bem a similaridade entre dois indivíduos. Interessante, não é?\n",
    "\n",
    "Como o KNN possui uma intuição simples, a matemática por trás dele também é fácil de compreender. Nós iremos entender como esta distância é calculada usando a métrica da **distância euclidiana**. Em seguida, aprenderemos a implementar o KNN para **classificação** e para **regressão** utilizando a biblioteca do **Sklearn**. Também teremos uma compreensão geral de como este modelo funciona a partir da apresentação de exemplos da execução do mesmo. E, por fim, aprenderemos a avaliá-lo seguindo a **métrica de acurácia** e alterando alguns parâmetros. Como a quantidade de vizinhos pesquisados. Então, no decorrer desse curso você irá aprender a utilizar o **KNN** para efetuar classsificação e/ou regressão em cima de um conjunto de elementos.\n",
    "\n",
    "Mas antes você precisa compreender como calcular a distância euclidiana que é uma das métricas de distância mais utilizadas para esse propósito.  \n",
    "A **distância euclidiana** é aprendida por muitos durante o ensino médio. E também aprendemos e conhecemos o Teorema de Pitágoras. Aquele que diz que, a soma dos catetos ao quadrado é igual a hipotenusa ao quadrado de um triângulo retângulo. Esta fórmula utilizada no Teorema de Pitágoras é exatamente igual a fórmula da distância euclidiana. \n",
    "\n",
    "Para tornar tudo um pouco mais claro irei mostrar um exemplo entre dois números quaisquer. Se quisermos mensurar a distância entre dois números. Iremos tomar como exemplo, os números 2 e 5. A distância entre o número 2 e 5 é a substração entre eles. Porém, essa subtração tem que ser efetuada com um certo cuidado. Pois, a ordem na qual iremos subtrair não deve fazer diferença no resultrado. Logo, se pegamos primeiro o ponto 5 e substrairmos o ponto 2, teremos a distância igual a 3. Mas, no caso contrário, se utilizamos primeiro o ponto 2 e formos subtrair o ponto 5 teremos uma distância negativa. E distâncias negativas não são interessantes até porquê elas não existem. Então, como poderíamos tentar de alguma maneira transformar essa distância que é negativa em uma distância positiva? \n",
    "\n",
    "Podemos usar a cédula do nosso Jupyter Notebook para efetuar esse cálculo. E também podemos examinar o que está ocorrendo. No caso, se utilizamos \n",
    "``2 - 5`` Como estou escrevendo na cédula. O Resultado será: ``-3`` E isso não é interessante porque não reflete uma distância. Mas, usando ``2 - 5`` Porém elevando essa substração ao quadrado, iremos utilizar a seguinte conta para elevar no python``(2-5)**2``, ao executar essa cédula teremos o **9** positivo. Até porque qualquqe número negativo, elevado ao quadrado irá se tornar positivo. Então, 2-5= -3, (-3)² = (-3 x (-3)) = 9\n",
    "\n",
    "Só que o 9 ainda não é a nossa distância final. Estamos tentando utilizar a distância euclidiana. E para omplentar a distância euclidiana iremos repetir a mesma linha anterior, só que iremos tirar a raiz quadrada deste número anterior. Que é extamente feito da seguinte maneira: Vamos reaproveitar a linha anterior, mas iremos colocá-la entre parênteses. E adicionar dois * e (0.5).\n",
    "``((2-5)**2)**(0.5)``\n",
    "\n",
    "Ou seja, estamos pegando exatamente o resultado da linha anterior e iremos elevá-la a 0.5. Elevar por 0.5 é igual a elevar a 1/2. E elevar a 1/2 é tirar a sua raiz quadrada. Logo, com esta linha: \n",
    "\n",
    "``((2-5)**2)**(0.5)``\n",
    "\n",
    "Teremos a subtração dos dois elementos. Estamos elevando essa substração ao quadrado e estamos tirando a raiz.Ou seja, no fim, estamos elevando ao quadrado para eliminar um valor negativo e tirando a raiz para voltar para o número normal. Então, o resultado desssa operação será extamente igual a 3. \n",
    "\n",
    "Uma vez que 2-5 ou 5-2, vão ter a distância de 3. Em termos de distância , a distância entre 5 e 2 não interessa a ordem, é **3**. E utilizando essa fórmula de elevar ao quadrado a substração e tirar a raiz, temos extqamente o valor positivo da distância.\n",
    "\n",
    "E, desta maneira, aprendemos a utilizar a distância entre dois pontos que possuem apenas uma coordenada. Como poderíamos utilizar algo para duas coordenadas? \n",
    "\n",
    "Vamos imaginar a situação em que temos uma fruta na qual temos a massa da fruta e a sua cor, o índice de cor. Então, queremos efetuar a distância entre dois elementos que possuem duas dimensões, ou seja, possuem duas características diferentes.\n",
    "\n",
    "Então, iremos efetuar a mesma técnica anterior, para cada um dos atributos diferentes da fruta. Ou seja, vamos supor que uma das frutas possui a sua massa 5 e a cor é extamente igual a ``0.75``. Iremos escrever isso em Python, como uma lista, e iremos armazenar em uma variável. Sendo uma variável, por exemplo:\n",
    "\n",
    "``\n",
    "a = [5, 0.75]\n",
    "b = [2, 0.50]\n",
    "``\n",
    "\n",
    "Ou seja, estamos representando na variável \"a\" uma fruta que possui massa igual a 5 e o índice de cor igual a 0.75. E a outra fruta na variável \"b\" que seria de massa igual 2 e de cor 0.50. Com isto, como conseguimos mensurar a distância entre essas duas frutas?\n",
    "\n",
    "A distância entre essas duas frutas é um cálculo bem parecido com o anterior onde iremos, para cada um dos atributos, ou seja, primeiramente para as massas e depois para as cores, iremos utilizar a distâncias entre dois pontos. \n",
    "\n",
    "Porém, considerando apenas cada um dos atributos separadamente. Como iremos fazer isto? \n",
    "\n",
    "Primeiro, iremos utilizar a mesma distância que vimos anteriormente que seria para separar dois pontos. Ou seja, temos o 5 e o 2, ou seja, 5 -2 que seria a massa da fruta a menos a massa da fruta b. Iremos elevar ao quadrado para evitar quaisquer possíveis números negativos. Então, elevado ao quadrado: \n",
    "\n",
    "``\n",
    "((5-2)**2)\n",
    "``\n",
    "\n",
    "E teremos o resultado dessa distância elevado ao quadrado. E iremos somá-lo com o próximo atributo, com a subtração ao quadrado do próximo atributo. Que seria:\n",
    "\n",
    "``\n",
    "((5-2)**2 + (0.75-0.50)**2\n",
    "``\n",
    "\n",
    "Ou seja, estamos separando as duas dimensões: a dimensão da massa e a dimensão da cor e estamos subtraindo elas. Subtraindo a massa de uma, menos a massa da outra eleva ao quadrado e soma esse resultado com o índice de cor de uma, menos o índice de cor da outra elevado ao quadrado. Assim, temos as distâncias entre as duas coordenadas elevadas ao quadrado.\n",
    "\n",
    "E, por fim, iremos elevar todo o resultado por ``0.5``.\n",
    "\n",
    "``\n",
    "((5-2) ** 2 + (0.75 - 0.50) ** 2) ** 0.5\n",
    "``\n",
    "\n",
    "Dessa forma, a raiz quadrada será efetuada pelas somas das diferenças ao quadrado.Dessamaneira, o resultado ao executar esta linha seria: ``((5-2) ** 2 + (0.75 - 0.50) ** 2) ** 0.5``\n",
    "\n",
    "Ou seja, estamos pegando as duas substrações ao quadrado, somando-as e tirando a raiz.E o resultado será algo em torno de: ``3.010398``. Em tela, temos um resultado maior.\n",
    "\n",
    "Mas, como ficou a ideia da distância nessa situação? \n",
    "\n",
    "Nesta situação, a distância entre os dois pontos sendo cada ponto composto por duas coordenadas, nós efetuamos a subtração de cada coordenada entre os pontos, elevamos essa subtração ao quadrado e tiramos a raiz quadrada do geral.\n",
    "\n",
    "Então, esta fórmula da distância entre dois pontos pode ser replicada para quantas coordenadas forem necessárias. Casode estarmos trabalhando com dados, cada coordenada é uma coluna do nosso dado. Então, iríamos efetuar a distância entre cada um dos elementos, de coluna à coluna, elevar ao quadrado esses resultados somar todos e tirar a raiz quadrada destes elementos.\n",
    "\n",
    "Então, dessa maneira, nós temos a distância entre dois pontos que podem ter quantas dimensões forem necessárias e iremos ter apenas um resultado que seria a distância. Então, desta maneira, conseguimos calcular a distância entre dois elementos sendo eles quaisquer tipos de dados que possuam quaisquer números de colunas e podemos mensurar a distância entre dois elementos. \n",
    "\n",
    "Então, estamos encerrando esta aula tendo apresentado a distância auclidiana que será determinante para a utilização do nosso método KNN. Na próxima aula, iremos apresentar a intuição do KNN para podermos implementá-lo em seguida. \n",
    "\n",
    "Até lá!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [5, 0.75]\n",
    "b = [2, 0.50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.010398644698074"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((5-2) ** 2 + (0.75 - 0.50) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN para Classificação\n",
    "\n",
    "Vamos apresentar o método KNN para classificação. Embora possar ser utilizado também em regressão. Sendo em outro momento, explicaremos mais aplicação do KNN em regressão. Agora, vamos iniciar adicionando a biblioteca **sklearn**. Seguinte código para importamos para classificação:\n",
    "\n",
    "``\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "``\n",
    "\n",
    "Caso, precise instalar no seu jupyter notebook a biblioteca:\n",
    "``\n",
    "conda install -c conda-forge scikit-learn  # instalar a biblioteca\n",
    "conda list scikit-learn # comando de teste para verificar a instalação\n",
    "``\n",
    "\n",
    "Após importar a biblioteca, vamos implementar esse método com seguinte código:\n",
    "\n",
    "``\n",
    "knn = KNeighborsClassifier (n_neighbors = 3)\n",
    "``\n",
    "\n",
    "Onde estamos instanciando na variável **knn** e implementar o método **KNeighborsClassifier**  adicionando o parâmetro **(n_neighbors = 3)**. Assim, verificaremos aqueles vizinhos mais próximos do dado de treino.\n",
    "\n",
    "---\n",
    "Não é recomendável esse método KNN quando os dados não for valor númerico! \n",
    "---\n",
    "\n",
    "Na sequência iremos ler os nossos dados, usando o seguinte código:\n",
    "\n",
    "``\n",
    "data = pd.read_table ('dataset/fruit_data_with_colors.txt')\n",
    "data\n",
    "``\n",
    "\n",
    "Agora, iremos criar duas variáveis para separar nossos atributos do rótulo.\n",
    "\n",
    "``\n",
    "x = data [['massa', 'altura', 'largura', 'color_score']]\n",
    "y = data ['fruit_label']\n",
    "``\n",
    "Depois de apresentar os dados de cada variável. Agora, que temos os atributos sepaarado dos rótulos, Iremos aplicar a função de divisão entre treino e teste.\n",
    "\n",
    "``\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split (x, y)\n",
    "``\n",
    "Com esses comandos podemos separar nosso conjunto de dados em dois subconjuntos de dados que um servirá para treino e outro para avaliar o modelo construído no treino que será o nosso teste. Que por padrão são divididos entre 75% de dados para treino e 25% para teste, aleatoriamente.\n",
    "\n",
    "Na cédula seguinte iremos adicionar o seguinte comando:\n",
    "``\n",
    "knn.fit (x_train, y_train)\n",
    "``\n",
    "Assim, iremos armazenar o nosso modelo que acabamos de construir na variável do nosso método. Na sequência, iremo avaliar o nosso modelo aplicaremos esse comando:\n",
    "\n",
    "``\n",
    "knn.score (x_test, y_test)\n",
    "``\n",
    "\n",
    "O resultado aparesentado demonstrar um desempenho do modelo bem inferior. O que recomenda-se fazer é aplicar a escala MinMax antes do KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completo\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "print(\"Setup completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier (n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit_label</th>\n",
       "      <th>fruit_name</th>\n",
       "      <th>fruit_subtype</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>80</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>80</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>76</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>braeburn</td>\n",
       "      <td>178</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>braeburn</td>\n",
       "      <td>172</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>braeburn</td>\n",
       "      <td>166</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>braeburn</td>\n",
       "      <td>172</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>braeburn</td>\n",
       "      <td>154</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>golden_delicious</td>\n",
       "      <td>164</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>golden_delicious</td>\n",
       "      <td>152</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>golden_delicious</td>\n",
       "      <td>156</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>golden_delicious</td>\n",
       "      <td>156</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>golden_delicious</td>\n",
       "      <td>168</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>162</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>162</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>160</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>156</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>140</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>cripps_pink</td>\n",
       "      <td>170</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>spanish_jumbo</td>\n",
       "      <td>342</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>spanish_jumbo</td>\n",
       "      <td>356</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>spanish_jumbo</td>\n",
       "      <td>362</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>204</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>140</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>160</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>158</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>210</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>selected_seconds</td>\n",
       "      <td>164</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>190</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>142</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>150</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>160</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>154</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>158</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>144</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>154</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>180</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>orange</td>\n",
       "      <td>turkey_navel</td>\n",
       "      <td>154</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>194</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>200</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>186</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>216</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>196</td>\n",
       "      <td>7.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>spanish_belsan</td>\n",
       "      <td>174</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>132</td>\n",
       "      <td>5.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>130</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>118</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>120</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116</td>\n",
       "      <td>6.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>152</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>lemon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>118</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fruit_label fruit_name     fruit_subtype  mass  width  height  color_score\n",
       "0             1      apple      granny_smith   192    8.4     7.3         0.55\n",
       "1             1      apple      granny_smith   180    8.0     6.8         0.59\n",
       "2             1      apple      granny_smith   176    7.4     7.2         0.60\n",
       "3             2   mandarin          mandarin    86    6.2     4.7         0.80\n",
       "4             2   mandarin          mandarin    84    6.0     4.6         0.79\n",
       "5             2   mandarin          mandarin    80    5.8     4.3         0.77\n",
       "6             2   mandarin          mandarin    80    5.9     4.3         0.81\n",
       "7             2   mandarin          mandarin    76    5.8     4.0         0.81\n",
       "8             1      apple          braeburn   178    7.1     7.8         0.92\n",
       "9             1      apple          braeburn   172    7.4     7.0         0.89\n",
       "10            1      apple          braeburn   166    6.9     7.3         0.93\n",
       "11            1      apple          braeburn   172    7.1     7.6         0.92\n",
       "12            1      apple          braeburn   154    7.0     7.1         0.88\n",
       "13            1      apple  golden_delicious   164    7.3     7.7         0.70\n",
       "14            1      apple  golden_delicious   152    7.6     7.3         0.69\n",
       "15            1      apple  golden_delicious   156    7.7     7.1         0.69\n",
       "16            1      apple  golden_delicious   156    7.6     7.5         0.67\n",
       "17            1      apple  golden_delicious   168    7.5     7.6         0.73\n",
       "18            1      apple       cripps_pink   162    7.5     7.1         0.83\n",
       "19            1      apple       cripps_pink   162    7.4     7.2         0.85\n",
       "20            1      apple       cripps_pink   160    7.5     7.5         0.86\n",
       "21            1      apple       cripps_pink   156    7.4     7.4         0.84\n",
       "22            1      apple       cripps_pink   140    7.3     7.1         0.87\n",
       "23            1      apple       cripps_pink   170    7.6     7.9         0.88\n",
       "24            3     orange     spanish_jumbo   342    9.0     9.4         0.75\n",
       "25            3     orange     spanish_jumbo   356    9.2     9.2         0.75\n",
       "26            3     orange     spanish_jumbo   362    9.6     9.2         0.74\n",
       "27            3     orange  selected_seconds   204    7.5     9.2         0.77\n",
       "28            3     orange  selected_seconds   140    6.7     7.1         0.72\n",
       "29            3     orange  selected_seconds   160    7.0     7.4         0.81\n",
       "30            3     orange  selected_seconds   158    7.1     7.5         0.79\n",
       "31            3     orange  selected_seconds   210    7.8     8.0         0.82\n",
       "32            3     orange  selected_seconds   164    7.2     7.0         0.80\n",
       "33            3     orange      turkey_navel   190    7.5     8.1         0.74\n",
       "34            3     orange      turkey_navel   142    7.6     7.8         0.75\n",
       "35            3     orange      turkey_navel   150    7.1     7.9         0.75\n",
       "36            3     orange      turkey_navel   160    7.1     7.6         0.76\n",
       "37            3     orange      turkey_navel   154    7.3     7.3         0.79\n",
       "38            3     orange      turkey_navel   158    7.2     7.8         0.77\n",
       "39            3     orange      turkey_navel   144    6.8     7.4         0.75\n",
       "40            3     orange      turkey_navel   154    7.1     7.5         0.78\n",
       "41            3     orange      turkey_navel   180    7.6     8.2         0.79\n",
       "42            3     orange      turkey_navel   154    7.2     7.2         0.82\n",
       "43            4      lemon    spanish_belsan   194    7.2    10.3         0.70\n",
       "44            4      lemon    spanish_belsan   200    7.3    10.5         0.72\n",
       "45            4      lemon    spanish_belsan   186    7.2     9.2         0.72\n",
       "46            4      lemon    spanish_belsan   216    7.3    10.2         0.71\n",
       "47            4      lemon    spanish_belsan   196    7.3     9.7         0.72\n",
       "48            4      lemon    spanish_belsan   174    7.3    10.1         0.72\n",
       "49            4      lemon           unknown   132    5.8     8.7         0.73\n",
       "50            4      lemon           unknown   130    6.0     8.2         0.71\n",
       "51            4      lemon           unknown   116    6.0     7.5         0.72\n",
       "52            4      lemon           unknown   118    5.9     8.0         0.72\n",
       "53            4      lemon           unknown   120    6.0     8.4         0.74\n",
       "54            4      lemon           unknown   116    6.1     8.5         0.71\n",
       "55            4      lemon           unknown   116    6.3     7.7         0.72\n",
       "56            4      lemon           unknown   116    5.9     8.1         0.73\n",
       "57            4      lemon           unknown   152    6.5     8.5         0.72\n",
       "58            4      lemon           unknown   118    6.1     8.1         0.70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table ('dataset/fruit_data_with_colors.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data [['mass','width', 'height', 'color_score']]\n",
    "y = data ['fruit_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>76</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>178</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>172</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>166</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>172</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>154</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>164</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>152</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>156</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>156</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>162</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>162</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>156</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>170</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>342</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>356</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>362</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>204</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>140</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>160</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>158</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>210</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>164</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>190</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>142</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>150</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>160</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>154</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>158</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>154</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>180</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>154</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>194</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>200</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>186</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>216</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>196</td>\n",
       "      <td>7.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>174</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>132</td>\n",
       "      <td>5.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>130</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>116</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>118</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>120</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>116</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>116</td>\n",
       "      <td>6.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>116</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>152</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>118</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass  width  height  color_score\n",
       "0    192    8.4     7.3         0.55\n",
       "1    180    8.0     6.8         0.59\n",
       "2    176    7.4     7.2         0.60\n",
       "3     86    6.2     4.7         0.80\n",
       "4     84    6.0     4.6         0.79\n",
       "5     80    5.8     4.3         0.77\n",
       "6     80    5.9     4.3         0.81\n",
       "7     76    5.8     4.0         0.81\n",
       "8    178    7.1     7.8         0.92\n",
       "9    172    7.4     7.0         0.89\n",
       "10   166    6.9     7.3         0.93\n",
       "11   172    7.1     7.6         0.92\n",
       "12   154    7.0     7.1         0.88\n",
       "13   164    7.3     7.7         0.70\n",
       "14   152    7.6     7.3         0.69\n",
       "15   156    7.7     7.1         0.69\n",
       "16   156    7.6     7.5         0.67\n",
       "17   168    7.5     7.6         0.73\n",
       "18   162    7.5     7.1         0.83\n",
       "19   162    7.4     7.2         0.85\n",
       "20   160    7.5     7.5         0.86\n",
       "21   156    7.4     7.4         0.84\n",
       "22   140    7.3     7.1         0.87\n",
       "23   170    7.6     7.9         0.88\n",
       "24   342    9.0     9.4         0.75\n",
       "25   356    9.2     9.2         0.75\n",
       "26   362    9.6     9.2         0.74\n",
       "27   204    7.5     9.2         0.77\n",
       "28   140    6.7     7.1         0.72\n",
       "29   160    7.0     7.4         0.81\n",
       "30   158    7.1     7.5         0.79\n",
       "31   210    7.8     8.0         0.82\n",
       "32   164    7.2     7.0         0.80\n",
       "33   190    7.5     8.1         0.74\n",
       "34   142    7.6     7.8         0.75\n",
       "35   150    7.1     7.9         0.75\n",
       "36   160    7.1     7.6         0.76\n",
       "37   154    7.3     7.3         0.79\n",
       "38   158    7.2     7.8         0.77\n",
       "39   144    6.8     7.4         0.75\n",
       "40   154    7.1     7.5         0.78\n",
       "41   180    7.6     8.2         0.79\n",
       "42   154    7.2     7.2         0.82\n",
       "43   194    7.2    10.3         0.70\n",
       "44   200    7.3    10.5         0.72\n",
       "45   186    7.2     9.2         0.72\n",
       "46   216    7.3    10.2         0.71\n",
       "47   196    7.3     9.7         0.72\n",
       "48   174    7.3    10.1         0.72\n",
       "49   132    5.8     8.7         0.73\n",
       "50   130    6.0     8.2         0.71\n",
       "51   116    6.0     7.5         0.72\n",
       "52   118    5.9     8.0         0.72\n",
       "53   120    6.0     8.4         0.74\n",
       "54   116    6.1     8.5         0.71\n",
       "55   116    6.3     7.7         0.72\n",
       "56   116    5.9     8.1         0.73\n",
       "57   152    6.5     8.5         0.72\n",
       "58   118    6.1     8.1         0.70"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x # exibir os dados dessa variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     2\n",
       "4     2\n",
       "5     2\n",
       "6     2\n",
       "7     2\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    1\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "20    1\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    3\n",
       "25    3\n",
       "26    3\n",
       "27    3\n",
       "28    3\n",
       "29    3\n",
       "30    3\n",
       "31    3\n",
       "32    3\n",
       "33    3\n",
       "34    3\n",
       "35    3\n",
       "36    3\n",
       "37    3\n",
       "38    3\n",
       "39    3\n",
       "40    3\n",
       "41    3\n",
       "42    3\n",
       "43    4\n",
       "44    4\n",
       "45    4\n",
       "46    4\n",
       "47    4\n",
       "48    4\n",
       "49    4\n",
       "50    4\n",
       "51    4\n",
       "52    4\n",
       "53    4\n",
       "54    4\n",
       "55    4\n",
       "56    4\n",
       "57    4\n",
       "58    4\n",
       "Name: fruit_label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # exibir os dados dessa variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # importando a função para treino e teste \n",
    "x_train, x_test, y_train, y_test = train_test_split (x, y) # promovendo o treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit (x_train, y_train) # Vamos armazernar esse modelo na variável knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score (x_test, y_test) # para avaliar o nosso modelo aplicaremos esse comando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processando os dados para o KNN\n",
    "\n",
    "Iremos utilizar o pré-processamento com Min e Max para tentar melhora o desempenho do modelo.\n",
    "Assim, iremos importar o Min e o Max com seguinte comando:\n",
    "\n",
    "``\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "``\n",
    "\n",
    "Execute e segue na próxima linha: \n",
    "\n",
    "``\n",
    "mm = MinMaxScaler ()\n",
    "``\n",
    "\n",
    "Teremos, assim, implementado nosso pré-processamento na variável **mm**. Agora iremos executar separadamente os dados de teste e dos treinos, transformando os dados na escala Min e Max.\n",
    "\n",
    "\n",
    "``\n",
    "x_train = mm.fit_transform (x_train)\n",
    "x_train\n",
    "\n",
    "``\n",
    "\n",
    "Depois os de teste:\n",
    "\n",
    "\n",
    "``\n",
    "x_test = mm.transform (x_test)\n",
    "x_test\n",
    "``\n",
    "\n",
    "Por que utilizamos comandos diferente entre o treino e teste?\n",
    "Porque quando utilizamos o comando ``x_train = mm.fit_transform (x_train)`` para treino e para teste basta usar o comando ``x_test = mm.transform (x_test)`` que será utilizado apenas com os dados de teste que não foram ainda transformados como os de treino. Para então, preparar os dados para construir o nosso modelo.\n",
    "\n",
    "Então, iremos retreinar nosso modelo, novamente, instanciando o método KNN: \n",
    "\n",
    "``\n",
    "knn = KNeighborsClassifier (n_neighbors = 3)\n",
    "``\n",
    "\n",
    "Na linha seguinte, iremos aplicar os comandos:\n",
    "\n",
    "``\n",
    "knn.fit (x_train, y_train)\n",
    "\n",
    "knn.score (x_test, y_test)\n",
    "``\n",
    "\n",
    "Logo, teremos como resultado 1.0, ou seja, 100% o modelo conseguiu predizer sobre o nosso dados. Essa função **Score** é capaz de efetuar a predição de acordo com os atributos de teste e comparar o resultado com os rótulos de testes. E se quisermos apenas predizer os nossos resultados. Digite o comando:\n",
    "\n",
    "``\n",
    "knn.predict (x_test)\n",
    "``\n",
    "\n",
    "Ao imprimir na tela o ``y_test``poderemos comparar o que você certo e errado no processo de predição.\n",
    "Tendo em vista que ao usar a função score essa comparação é efetuada automaticamente, já retornada com a probabilidade de acerto.\n",
    "\n",
    "Na próxima aula, iremos ver o KNN para Regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mm.fit_transform (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31468531, 0.28947368, 0.50769231, 1.        ],\n",
       "       [0.43356643, 0.39473684, 1.        , 0.38235294],\n",
       "       [0.30769231, 0.36842105, 0.46153846, 0.61764706],\n",
       "       [0.27272727, 0.39473684, 0.50769231, 0.58823529],\n",
       "       [0.3006993 , 0.42105263, 0.49230769, 0.76470588],\n",
       "       [0.27972028, 0.47368421, 0.53846154, 0.23529412],\n",
       "       [0.36363636, 0.47368421, 0.64615385, 0.58823529],\n",
       "       [1.        , 1.        , 0.8       , 0.44117647],\n",
       "       [0.27272727, 0.34210526, 0.53846154, 0.55882353],\n",
       "       [0.33566434, 0.42105263, 0.46153846, 0.88235294],\n",
       "       [0.14685315, 0.02631579, 0.61538462, 0.38235294],\n",
       "       [0.25874126, 0.34210526, 0.6       , 0.47058824],\n",
       "       [0.36363636, 0.57894737, 0.43076923, 0.        ],\n",
       "       [0.27972028, 0.42105263, 0.52307692, 0.73529412],\n",
       "       [0.30769231, 0.39473684, 0.56923077, 0.32352941],\n",
       "       [0.14685315, 0.07894737, 0.63076923, 0.32352941],\n",
       "       [0.27272727, 0.31578947, 0.47692308, 0.85294118],\n",
       "       [0.34965035, 0.42105263, 0.49230769, 0.02941176],\n",
       "       [0.3006993 , 0.44736842, 0.47692308, 0.70588235],\n",
       "       [0.3986014 , 0.44736842, 0.63076923, 0.44117647],\n",
       "       [0.1958042 , 0.        , 0.72307692, 0.41176471],\n",
       "       [0.97902098, 0.89473684, 0.8       , 0.47058824],\n",
       "       [0.13986014, 0.05263158, 0.53846154, 0.38235294],\n",
       "       [0.35664336, 0.34210526, 0.58461538, 0.97058824],\n",
       "       [0.22377622, 0.39473684, 0.47692308, 0.82352941],\n",
       "       [0.15384615, 0.05263158, 0.67692308, 0.44117647],\n",
       "       [0.32167832, 0.44736842, 0.55384615, 0.41176471],\n",
       "       [0.        , 0.        , 0.        , 0.64705882],\n",
       "       [0.28671329, 0.34210526, 0.53846154, 0.58823529],\n",
       "       [0.28671329, 0.36842105, 0.58461538, 0.52941176],\n",
       "       [0.13986014, 0.02631579, 0.63076923, 0.41176471],\n",
       "       [0.18881119, 0.05263158, 0.64615385, 0.35294118],\n",
       "       [0.02797203, 0.05263158, 0.09230769, 0.58823529],\n",
       "       [0.23776224, 0.26315789, 0.52307692, 0.47058824],\n",
       "       [0.26573427, 0.18421053, 0.69230769, 0.38235294],\n",
       "       [0.32867133, 0.47368421, 0.6       , 0.85294118],\n",
       "       [0.29370629, 0.34210526, 0.55384615, 0.5       ],\n",
       "       [0.13986014, 0.07894737, 0.69230769, 0.35294118],\n",
       "       [0.41258741, 0.36842105, 0.96923077, 0.32352941],\n",
       "       [0.22377622, 0.23684211, 0.47692308, 0.38235294],\n",
       "       [0.23076923, 0.47368421, 0.58461538, 0.47058824],\n",
       "       [0.33566434, 0.34210526, 0.55384615, 0.97058824],\n",
       "       [0.46853147, 0.52631579, 0.61538462, 0.67647059],\n",
       "       [0.44755245, 0.44736842, 0.8       , 0.52941176]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = mm.transform (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29370629,  0.31578947,  0.52307692,  0.64705882],\n",
       "       [ 0.27272727,  0.36842105,  0.49230769,  0.67647059],\n",
       "       [ 0.93006993,  0.84210526,  0.83076923,  0.47058824],\n",
       "       [ 0.29370629,  0.44736842,  0.53846154,  0.79411765],\n",
       "       [ 0.41958042,  0.39473684,  0.87692308,  0.38235294],\n",
       "       [ 0.40559441,  0.68421053,  0.50769231, -0.11764706],\n",
       "       [ 0.27972028,  0.5       ,  0.47692308,  0.29411765],\n",
       "       [ 0.26573427,  0.47368421,  0.50769231,  0.29411765],\n",
       "       [ 0.48951049,  0.39473684,  0.95384615,  0.35294118],\n",
       "       [ 0.38461538,  0.36842105,  0.8       ,  0.38235294],\n",
       "       [ 0.01398601,  0.02631579,  0.04615385,  0.64705882],\n",
       "       [ 0.34265734,  0.39473684,  0.93846154,  0.38235294],\n",
       "       [ 0.03496503,  0.10526316,  0.10769231,  0.61764706],\n",
       "       [ 0.01398601,  0.        ,  0.04615385,  0.52941176],\n",
       "       [ 0.13986014,  0.13157895,  0.56923077,  0.38235294]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier (n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, 1, 4, 1, 1, 1, 4, 3, 2, 4, 2, 2, 4], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    3\n",
       "42    3\n",
       "24    3\n",
       "20    1\n",
       "47    4\n",
       "0     1\n",
       "15    1\n",
       "14    1\n",
       "46    4\n",
       "45    4\n",
       "6     2\n",
       "48    4\n",
       "3     2\n",
       "5     2\n",
       "55    4\n",
       "Name: fruit_label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN para Regressão\n",
    "\n",
    "Olá\n",
    "\n",
    "O KNN para regressão é bem parecido com o KNN para classsificação. Poucas mudanças serão feitas aqui. \n",
    "Primeiro, a intuição do KNN para regressão é um pouco diferente visto que não iremos escolher os vizinhos mais próximos e pegar a classe da maioria dos vizinhos mais próximos. No caso da regressão iremos efetuar uma média do rótulo dos vizinhos mais proximos. Ou seja, o processo de cálculo da distância é o mesmo; porém, na horaa de aplicar o rótulo do novo elemento iremos utilizar somente a média dos rótulos dos vizinhos mais próximos. \n",
    "\n",
    "Então, iremos importar o método KNN para regressão:\n",
    "\n",
    "``\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "``\n",
    "\n",
    "Ao executar essa linha teremos importado o KNN para regressão. E para utilizá-lo iremos instanciar na variável **knn**, da mesma maneira que fizemos anteriormente.\n",
    "\n",
    "``\n",
    "knn = KNeighborsRegressor (n_neighbors = 3)\n",
    "``\n",
    "\n",
    "Isto que dizer que será igual a média dos três vizinhos mais próximos. Executando essa linha teremos o KNN implementado. Então, para executar alguma predição diferente neste KNN, iremos utilizar um \"dataset\", um conjunto de dados diferente do que vinhamos utilizando até então. porque o conjunto de dados que vinhamos utilizando é um conjunto de dados para classsificação. E este novo conjunto de dados é referente ao preço de casa de diferentes regiões de Boston.\n",
    "\n",
    "``\n",
    "from sklearn.datasets import load_boston\n",
    "``\n",
    "\n",
    "Em seguida, iremos armazenar esse conjunto de dados na variável data.`` data = load_boston ()`` Se imprimir na tela, veremos que esse conjunto de dados é um dicionário. E para ver melhor esse conjunto de dados, vamos utilizar seguinte código::\n",
    "\n",
    "``\n",
    "X, y = load_boston (return_X_y = True)\n",
    "``\n",
    "Desta maneira, load_boston irá retornar tanto os atributos quanto os rótulos já separados. Se imprimimos o X, teremos os atributos separados dos rótulos. Ao utilizamos o comando: ``X.shape`` veremos que temos 13 colunas e um total de 506 linhas. O mesmo pode ser aplicado ao y: ``y.shape`` teremos 506 rótulos que seriam os preços das casas de cada região de Boston.\n",
    "\n",
    "Então, para entender melhor como funciona esse dataset, você pode utilizar o seguinte comando:\n",
    "\n",
    "``\n",
    "print(load_boston()['DESCR'])\n",
    "``\n",
    "\n",
    "Esse \"DESCR\", seria _description_ ou descrição em português. Ao executar essa linha, teremos impresso as informações desse dataset e o valor médio do preço da casa.\n",
    "\n",
    "Agora, iremos separar o nosso conjunto de dados na divisão de treino e teste. Logo, iremos importar a separação em treino e teste:\n",
    "\n",
    "``\n",
    "from sklearn.model_selection import train_test_split\n",
    "``\n",
    "\n",
    "E na sequência, implementar a divisão do nosso conjunto de dados para treino e teste. \n",
    "\n",
    "``\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y)\n",
    "``\n",
    "\n",
    "Rodando essa função temos que, a divisão em treino e teste foi efetuada.\n",
    "A partir desse momento iremos executar o método KNN para regressão.\n",
    "\n",
    "``\n",
    "knn.fit (X_train, y_train)\n",
    "``\n",
    "\n",
    "Depois iremos aplicar a função score:\n",
    "\n",
    "``\n",
    "knn.score (X_test, y_test)\n",
    "``\n",
    "\n",
    "Teremos o resultado da regressão. Então, escalando nos atributos poderemos atingir um resultado melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor (n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston (return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(load_boston()['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.478002411070159"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mm.fit_transform (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = mm.transform (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor (n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781766008479512"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.9       , 19.1       , 12.26666667,  9.23333333, 31.6       ,\n",
       "       10.8       , 20.33333333, 21.36666667, 31.23333333, 39.8       ,\n",
       "       15.43333333, 20.5       , 22.43333333, 29.83333333, 24.23333333,\n",
       "       20.43333333, 20.43333333, 13.56666667, 10.6       , 21.36666667,\n",
       "       18.76666667, 10.4       , 20.16666667, 49.5       , 14.1       ,\n",
       "       14.3       , 19.6       , 28.4       , 23.56666667, 13.4       ,\n",
       "       21.03333333, 45.5       , 30.1       , 20.8       , 28.43333333,\n",
       "       20.86666667, 36.7       , 18.4       , 13.56666667, 22.33333333,\n",
       "       22.8       ,  9.76666667, 29.83333333, 25.83333333, 28.06666667,\n",
       "       21.56666667, 16.03333333, 22.03333333, 22.4       , 21.73333333,\n",
       "       12.13333333, 22.83333333, 22.73333333, 19.53333333, 26.73333333,\n",
       "       20.33333333, 31.6       , 18.2       , 18.53333333, 20.23333333,\n",
       "       29.46666667, 10.33333333, 15.06666667, 14.33333333, 10.33333333,\n",
       "       19.56666667, 24.5       , 20.73333333, 37.06666667, 20.23333333,\n",
       "       32.        ,  9.03333333, 40.56666667, 23.53333333, 33.16666667,\n",
       "       20.6       , 14.13333333, 23.33333333, 21.56666667, 22.53333333,\n",
       "       24.26666667, 15.96666667, 34.63333333, 24.03333333, 22.96666667,\n",
       "        9.2       , 15.13333333, 32.06666667, 26.13333333, 15.96666667,\n",
       "       24.43333333, 18.4       , 22.43333333, 21.66666667, 23.16666667,\n",
       "       26.8       , 34.73333333, 22.9       , 21.1       , 21.83333333,\n",
       "       19.2       , 19.36666667, 21.56666667, 18.83333333, 14.26666667,\n",
       "       13.4       , 28.5       ,  9.03333333, 10.8       , 27.73333333,\n",
       "       17.93333333, 14.        , 30.9       , 20.9       , 31.53333333,\n",
       "       16.1       , 13.2       , 24.03333333, 19.96666667, 26.86666667,\n",
       "       19.93333333, 18.76666667, 29.83333333, 35.46666667, 31.23333333,\n",
       "       24.93333333, 19.9       ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
