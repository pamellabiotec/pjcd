{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a Machine Learning\n",
    "\n",
    "## Introdução ao processamento da linguagem natural\n",
    "\n",
    "Olá, seja bem-vindo!\n",
    "\n",
    "A capacidade de comunicar-se é uma das principais características que nos permitem viver em sociedade. E a tecnologia é um aspecto que vem gradativamente evoluindo e facilitando a forma com que nos comunicamos.\n",
    "\n",
    "Nesse sentindo, está cada vez mais fácil usar aplicativos para troca de mensagens e já existem, inclusive, sistemas que usam machine learning para simular uma conversa como se fossem uma pessoa respondendo. Quanta inovação, não é mesmo?!\n",
    "\n",
    "Esses sistemas são chamados de _chatbots_. Eles já estão presentes em diversos sites de comércio eletrônico e em serviços de atendimento de várias empresas. Neles o usuário geralmente é apresentado a um chat com algumas opções de serviço e o assistente virtual vai direcionando o utilizador para determinado setor de atendimento da empresa, com base em suas respostas em mensagens de texto.\n",
    "\n",
    "E como esses sistemas fazem isso? Como eles conseguem entender as mensagens do usuário?\n",
    "\n",
    "Bem, por trás dessa tecnologia existe uma área da computação que é chamada de Processamento da Linguagem Natural, tem o objetivo de fazer com que um sistema consiga entender a linguagem dos humanos, ou seja, entender o que nós escrevemos ou falamos.\n",
    "\n",
    "O Processamento da Linguagem Natural, também representado pela sigla PLN, ou NLP do inglês _Natural Language Processing_, utiliza conceitos baseados em linguística e regras gramaticais para a construção de algoritmos, que possam extrair alguma informação ou entendimento.\n",
    "\n",
    "Dentre as aplicações do PNL pode-se destacar a **sumarização** ou resumo de textos, que permite, por exemplo, captar apenas as ideias principais que contém o sentido de um texto ou de um livro. Outra aplicação são os aplicativos de **tradução** que utilizam o reconhecimento de voz do usuário para traduzir uma frase. O PNL pode ser aplicado ainda em **recuperação de informação**, **chatbots**, entre outras utilidades.\n",
    "\n",
    "Caro, cursista, agora você será apresentado, de fato, a alguns conceitos importantes no aprendizado de PLN. São eles: **Corpus**, **Tokenização**, e **Stop words**.\n",
    "\n",
    "O **Corpus** é representado por um conjunto de textos escritos em um determinado idioma, que foram manualmente anotados e servirá de validação para as análises que serão realizadas.\n",
    "\n",
    "Já a **Tokenização**, refere-se a divisão de um texto em partes menores que representam as palavras, ou também chamadas de tokens, e podem ser separadas por espaços, vírgulas ou pontuações.\n",
    "\n",
    "Por último, mas não menos importante, os **Stop words**. Eles são palavras que geralmente são removidas no início do processamento dos textos para acelerar esse processo, porém a retirada dessas palavras não afeta a compreensão do mesmo.\n",
    "\n",
    "Após a definição destes conceitos, para que você os entenda melhor e consiga utilizá-los em seu cotidiano de programador é importante acompanhar a aplicação do que foi estudado.\n",
    "\n",
    "Nesse caso, será usando uma biblioteca do python chamada de NLTK que significa _Natural Language Toolkit_. Então, com o ambiente do jupyter aberto, faça inicialmente o download da biblioteca com o comando ``! pip install nltk``.\n",
    "\n",
    "Feito isso, agora você deve importar a biblioteca com o comando ``import nltk``, e então será necessário fazer o download das stop words em português. Primeiramente escreva o comando ``nltk.download('stopwords')`` para fazer o download das stop words.\n",
    "\n",
    "Então, escreva o comando ``stopwords = nltk.corpus.stopwords.words('portuguese')`` para obter as stop words em português.\n",
    "\n",
    "Para que você consiga ter uma ideia de quais são as stop words, utilize o comando:  ``print(stopwords)``.\n",
    "\n",
    "Assim poderá fazer um teste simples para remover as stop words de uma frase. Então, utilize o módulo “word_tokenize” que como o nome sugere, retorna uma lista de tokens para um texto dado como entrada. Para fazer isso, você deve importar o módulo através do comando ``from nltk.tokenize import word_tokenize``. É necessário também usar o comando ``nltk.download(‘punkt’)``, pois estes são os recursos necessários para fazer a **tokenização**.\n",
    "\n",
    "Logo, você poderá criar uma variável frase, que aqui será ``frase = 'Eu dirijo devagar porque nós queremos ver os animais.'``. Então para dividir a frase em tokens, é só passar a variável frase para o módulo “word_tokenize”, fazendo ``tokens = word_tokenize(frase)`` e escrevendo ``print(tokens)`` caso queira entender como os tokens foram criados.\n",
    "\n",
    "Com a lista de tokens criada, faça um **loop simples** usando a condição if para não imprimir na tela a stopword caso ela exista em cada token. Então na primeira linha do loop escreva ``for t in tokens:`` seguido da indentação, continue com ``if t not in stopwords:``, finalizando com mais uma indentação e ``print(t)``.\n",
    "\n",
    "Com isso é possível perceber que foram impressos sete tokens, são eles: eu, dirijo, devagar, porque, queremos, ver, animais. Dentre eles não aparece na tela os tokens “nós” e “os”, isso significa que estes dois são as stop words da variável frase, e ao removê-las a frase ainda continua fazendo sentido.\n",
    "\n",
    "Interessante, não é mesmo?\n",
    "\n",
    "Outra técnica muito importante na análise de textos, é a **identificação da frequência** e **importância** que uma palavra pode ter em um texto.\n",
    "\n",
    "Existe um cálculo estatístico chamado de TF-IDF, essa sigla vem do inglês _Term Frequency – Inverse Document Frequency_, e quer dizer: **Frequência do termo - Frequência Inversa do Documento**. Este cálculo identifica a importância que um termo tem em um texto, ou seja, ele permite que você descubra quais termos são mais relevantes em um dado texto ou documento.\n",
    "\n",
    "Em linhas gerais o TF-IDF busca atribuir um valor que representa um peso para definir a importância do termo em um documento, com base na frequência em que ele ocorre (TF), compensando, porém, esse peso, caso a ocorrência desse termo seja muito grande (IDF).\n",
    "\n",
    "Em resumo, se um termo aparece algumas vezes no texto, o valor do TF-IDF aumenta, significando que aquela palavra é importante, porém se esse termo se repete bastante, esse valor é compensado, e a importância dele é diminuída.\n",
    "\n",
    "Deu pra entender o TF-IDF? Agora acompanhe um exemplo prático, para melhorar sua compreensão sobre esse assunto, utilizando o **módulo TfidfVectorizer** da biblioteca sklearn para calcular os valores de TF-IDF de um texto.\n",
    "\n",
    "Com o ambiente do jupyter aberto, primeiramente importe o módulo através do comando ``from sklearn.feature_extraction.text import TfidfVectorizer``, e importe também a biblioteca pandas, fazendo ``import pandas as pd``.\n",
    "\n",
    "Em seguida crie uma variável texto1 para aplicar o cálculo do TF-IDF. Para isso, basta escrever texto1 = 'A matemática é muito importante para compreendermos como a natureza funciona'.\n",
    "\n",
    "Então você poderá instanciar o módulo fazendo ``tf_idf = TfidfVectorizer()``. \n",
    "\n",
    "Feito isso use o método ``fit_transform`` que retorna uma matriz com os termos e os scores associados, passando o texto1 com o conteúdo. Em seguida para colocar no formato de um array use o comando ``vetor = vetor.todense()``.\n",
    "\n",
    "Para obter os nomes das palavras mapeadas, pode usar o código ``nomes = tf_idf.get_feature_names()``.\n",
    "\n",
    "Assim é possível associar os scores aos nomes das palavras e o resultado pode ser criado em um dataframe, fazendo ``df = pd.DataFrame(vetor, columns=nomes)``.\n",
    "\n",
    "Analisando o resultado dos **scores das palavras** você pode notar que o TF-IDF deu o mesmo score para todos os termos, pois só existe uma ocorrência de cada termo.\n",
    "\n",
    "Porém se criar outro texto com o seguinte conteúdo: ``texto2 = 'A matemática é incrível, quanto mais estudo matemática, mais eu consigo aprender matemática'``, e repetir os procedimentos feitos anteriormente para o texto1, poderá obter scores diferentes.\n",
    "\n",
    "Logo perceberá que a palavra ‘matemática’ recebe o maior score, pois ocorre 3 vezes no texto. A palavra ‘mais’ recebe o segundo maior score, pois só aparece 2 vezes, e o restante recebe o mesmo score, pois só aparecem 1 vez.\n",
    "\n",
    "Muita coisa, não é mesmo?\n",
    "\n",
    "Até aqui você pôde aprender sobre o Processamento da Linguagem Natural de uma forma introdutória, e conhecer alguns conceitos importantes para o processamento de texto como, Corpus, Stopwords e Tokenização. Também entendeu como é realizado o cálculo TFIDF para verificar a importância que um termo tem dentro de um texto. Além de tudo isso, pôde acompanhar a aplicação desses conceitos utilizando Python. Então, agora é com você!\n",
    "\n",
    "Tente praticar e resolver os exercícios propostos para aprofundar seus conhecimentos.\n",
    "\n",
    "Bons estudos e até mais!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\clar1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\clar1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'Eu dirijo devagar porque não queremos ver os animais.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(frase) #tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', 'dirijo', 'devagar', 'porque', 'não', 'queremos', 'ver', 'os', 'animais', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu\n",
      "dirijo\n",
      "devagar\n",
      "porque\n",
      "queremos\n",
      "ver\n",
      "animais\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    if t not in stopwords:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o Módulo TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto1 = 'A matemática é muito importante para compreendermos como a natureza funciona.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.35355339059327373\n",
      "  (0, 6)\t0.35355339059327373\n",
      "  (0, 0)\t0.35355339059327373\n",
      "  (0, 1)\t0.35355339059327373\n",
      "  (0, 7)\t0.35355339059327373\n",
      "  (0, 3)\t0.35355339059327373\n",
      "  (0, 5)\t0.35355339059327373\n",
      "  (0, 4)\t0.35355339059327373\n"
     ]
    }
   ],
   "source": [
    "vetor = tf_idf.fit_transform([texto1])\n",
    "print(vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
      "  0.35355339 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "vetor = vetor.todense()\n",
    "print(vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       como  compreendermos  funciona  importante  matemática     muito  \\\n",
      "0  0.353553        0.353553  0.353553    0.353553    0.353553  0.353553   \n",
      "\n",
      "   natureza      para  \n",
      "0  0.353553  0.353553  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(vetor, columns=nomes)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando em novo texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto2 = 'A matemática é incrível, quanto mais estudo matemática, mais eu consigo aprender matemática'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "vetor2 = tf_idf.fit_transform([texto2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor2 = vetor2.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aprender   consigo    estudo        eu  incrível      mais  matemática  \\\n",
      "0  0.229416  0.229416  0.229416  0.229416  0.229416  0.458831    0.688247   \n",
      "\n",
      "     quanto  \n",
      "0  0.229416  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(vetor2, columns=nomes)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
